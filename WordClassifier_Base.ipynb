{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ea197c5e5b2ff80f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:06:06.166926Z",
     "start_time": "2024-07-07T15:06:06.163948Z"
    }
   },
   "outputs": [],
   "source": [
    "# spell correction\n",
    "# normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "caf8ed523a74f5cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:06:06.373558Z",
     "start_time": "2024-07-07T15:06:06.367454Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "from hazm import InformalNormalizer, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3d3729624c59f9b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:06:06.618712Z",
     "start_time": "2024-07-07T15:06:06.611570Z"
    }
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:06:07.624054Z",
     "start_time": "2024-07-07T15:06:07.108927Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepared_data: pd.DataFrame = pd.read_csv('datasets/taghche.csv')\n",
    "prepared_data = prepared_data[['comment', 'bookname']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c7c751dd317e7678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:06:07.639704Z",
     "start_time": "2024-07-07T15:06:07.631864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>bookname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اسم کتاب   No one writes to the Colonel\\nترجمش...</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طاقچه عزیز،نام کتاب\"کسی به سرهنگ نامه نمینویسد...</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بنظرم این اثر مارکز خیلی از صد سال تنهایی که ب...</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>به نظر کتاب خوبی میومد اما من از ترجمش خوشم نی...</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>کتاب خوبی است</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0  اسم کتاب   No one writes to the Colonel\\nترجمش...   \n",
       "1  طاقچه عزیز،نام کتاب\"کسی به سرهنگ نامه نمینویسد...   \n",
       "2  بنظرم این اثر مارکز خیلی از صد سال تنهایی که ب...   \n",
       "3  به نظر کتاب خوبی میومد اما من از ترجمش خوشم نی...   \n",
       "4                                      کتاب خوبی است   \n",
       "\n",
       "                            bookname  \n",
       "0  سرهنگ کسی ندارد برایش نامه بنویسد  \n",
       "1  سرهنگ کسی ندارد برایش نامه بنویسد  \n",
       "2  سرهنگ کسی ندارد برایش نامه بنویسد  \n",
       "3  سرهنگ کسی ندارد برایش نامه بنویسد  \n",
       "4  سرهنگ کسی ندارد برایش نامه بنویسد  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bd59dc1f5b219367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:06:09.709450Z",
     "start_time": "2024-07-07T15:06:09.703192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[\\ufeff!\"#()*,-./:\\[\\]«»،؛؟۰۱۲۳۴۵۶۷۸۹…$ًٌٍَُِّْءٰٔ﷼]', re.UNICODE)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_stop_words = ''\n",
    "with open('stopwords/chars.txt', 'r', encoding='utf-8') as file:\n",
    "    chars_stop_words = ''.join(file.read().splitlines())\n",
    "\n",
    "chars_stop_words = chars_stop_words.replace('[', '\\[')\n",
    "chars_stop_words = chars_stop_words.replace(']', '\\]')\n",
    "chars_pattern = re.compile(f'[{chars_stop_words}]')\n",
    "chars_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f3dfc0ccaed98d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:06:10.065940Z",
     "start_time": "2024-07-07T15:06:10.058052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[😀-🙏🌀-🗿🚀-\\U0001f6ff\\U0001f1e0-🇿]+', re.UNICODE)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "emojis_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\")\n",
    "emojis_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b5a5f26fadc093b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:06:11.832560Z",
     "start_time": "2024-07-07T15:06:11.813027Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    global chars_pattern, emojis_pattern\n",
    "    \n",
    "    text = chars_pattern.sub(r' ', text)\n",
    "    text = emojis_pattern.sub(r' ', text)\n",
    "    return informal_normalizer_function(text)\n",
    "\n",
    "\n",
    "# customizing InformalNormalizer().normalize()\n",
    "# For seeing differences, you can see InformalNormalizer().normalize() method\n",
    "def informal_normalizer_function(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    informal_normalizer = InformalNormalizer()\n",
    "    text = Normalizer.normalize(informal_normalizer, text)\n",
    "    sents = [\n",
    "        informal_normalizer.word_tokenizer.tokenize(sentence)\n",
    "        for sentence in informal_normalizer.sent_tokenizer.tokenize(text)\n",
    "    ]\n",
    "\n",
    "    normalized = [[informal_normalizer.normalized_word(word)[0] for word in sent] for sent in sents]\n",
    "    normalized = np.array(normalized, dtype=object)\n",
    "    return np.hstack(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cf3e30caaf92a2c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:08:13.894553Z",
     "start_time": "2024-07-07T15:06:13.870195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b075e62be94a7190f0f0fce4aafb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prepared_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mprepared_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py:805\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py:800\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    799\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[114], line 6\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m chars_pattern\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m emojis_pattern\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minformal_normalizer_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[114], line 14\u001b[0m, in \u001b[0;36minformal_normalizer_function\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minformal_normalizer_function\u001b[39m(text):\n\u001b[1;32m     12\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(text)\n\u001b[0;32m---> 14\u001b[0m     informal_normalizer \u001b[38;5;241m=\u001b[39m \u001b[43mInformalNormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     text \u001b[38;5;241m=\u001b[39m Normalizer\u001b[38;5;241m.\u001b[39mnormalize(informal_normalizer, text)\n\u001b[1;32m     16\u001b[0m     sents \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m         informal_normalizer\u001b[38;5;241m.\u001b[39mword_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sentence)\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m informal_normalizer\u001b[38;5;241m.\u001b[39msent_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[1;32m     19\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/hazm/informal_normalizer.py:41\u001b[0m, in \u001b[0;36mInformalNormalizer.__init__\u001b[0;34m(self, verb_file, word_file, seperation_flag, **kargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39milemmatizer \u001b[38;5;241m=\u001b[39m InformalLemmatizer()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstemmer \u001b[38;5;241m=\u001b[39m Stemmer()\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msent_tokenizer \u001b[38;5;241m=\u001b[39m SentenceTokenizer()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_tokenizer \u001b[38;5;241m=\u001b[39m WordTokenizer()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/hazm/normalizer.py:54\u001b[0m, in \u001b[0;36mNormalizer.__init__\u001b[0;34m(self, correct_spacing, remove_diacritics, remove_specials_chars, decrease_repeated_chars, persian_style, persian_numbers, unicodes_replacement, seperate_mi)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslation_dst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mیککیییکیبقویتتبتتتبحاوویتتبتتتبحححچدددددددددررررررررسسسصصطعففففففققکککککگگگگگللللنننننهچهههوووووووووییییییهدرشضغهبببببببححددرسعععففکککممنننلررسححسرحاایییووییحسسکببجطفقلمییرودصگویزعکبپتریفقنااببببپپپپببببتتتتتتتتتتتتففففححححححححچچچچچچچچددددددددژژررککککگگگگگگگگگگگگننننننههههههههههییییءاااووااییییااببببتتتتثثثثججججححححخخخخددذذررززسسسسششششصصصصضضضضططططظظظظععععغغغغففففققققککککللللممممننننههههوویییییییکی\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_correct_spacing \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decrease_repeated_chars:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mWordTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_verb_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mwords\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persian_number:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/hazm/word_tokenizer.py:103\u001b[0m, in \u001b[0;36mWordTokenizer.__init__\u001b[0;34m(self, words_file, verbs_file, join_verb_parts, join_abbreviations, separate_emoji, replace_links, replace_ids, replace_emails, replace_numbers, replace_hashtags)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# NOTE: python2.7 does not support unicodes with \\w\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhashtag_repl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTAG \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;241m0\u001b[39m]: (item[\u001b[38;5;241m1\u001b[39m], item[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwords_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_file\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join_verb_parts:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_verbs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mام\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mای\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mنخواهند_شد\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m     }\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/hazm/utils.py:48\u001b[0m, in \u001b[0;36mwords_list\u001b[0;34m(words_file)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mopen(words_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m words_file:\n\u001b[1;32m     47\u001b[0m     items \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m words_file]\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     49\u001b[0m         (item[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m(item[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mtuple\u001b[39m(item[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     52\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/hazm/utils.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mopen(words_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m words_file:\n\u001b[1;32m     47\u001b[0m     items \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m words_file]\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m---> 49\u001b[0m         (\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mint\u001b[39m(item[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mtuple\u001b[39m(item[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     52\u001b[0m     ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prepared_data['comment'] = prepared_data['comment'].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d673b6c484593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:08:13.897085Z",
     "start_time": "2024-07-07T15:08:13.896909Z"
    }
   },
   "outputs": [],
   "source": [
    "prepared_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ddb3e98c34282776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:00:44.957111Z",
     "start_time": "2024-07-07T15:00:44.134230Z"
    }
   },
   "outputs": [],
   "source": [
    "ALL_PARTS_LEN = 18\n",
    "crawled_data: pd.DataFrame = pd.read_csv('datasets/books data/books_data_part_1.csv')\n",
    "for i in range(2, ALL_PARTS_LEN + 1):\n",
    "    crawled_data = pd.concat([crawled_data, pd.read_csv(f'datasets/books data/books_data_part_{i}.csv')],\n",
    "                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9fca1a1b98601fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:00:45.062517Z",
     "start_time": "2024-07-07T15:00:44.959340Z"
    }
   },
   "outputs": [],
   "source": [
    "# sort authors in order to drop duplicates\n",
    "new_author_function = lambda x: ' $ '.join(sorted(str(x).split(' $ ')))\n",
    "crawled_data['author'] = crawled_data['author'].apply(new_author_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "542c164cfa0c2a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:00:45.511483Z",
     "start_time": "2024-07-07T15:00:45.318093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2256 duplicates.\n"
     ]
    }
   ],
   "source": [
    "before_dropping = len(crawled_data)\n",
    "crawled_data = crawled_data.drop_duplicates()\n",
    "print(f'Dropped {before_dropping - len(crawled_data)} duplicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6e49fc20f57ef364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:00:54.635362Z",
     "start_time": "2024-07-07T15:00:46.479339Z"
    }
   },
   "outputs": [],
   "source": [
    "crawled_data = crawled_data.groupby(['name', 'author']).agg({'translator': set, 'publisher': set}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ff1ac4627815bd4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:03:39.033605Z",
     "start_time": "2024-07-07T15:03:38.894405Z"
    }
   },
   "outputs": [],
   "source": [
    "new_author_function = lambda x: set(x.split(' $ '))\n",
    "crawled_data['author'] = crawled_data['author'].apply(new_author_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cc7ff06613b1d25d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:03:39.547301Z",
     "start_time": "2024-07-07T15:03:39.526647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>translator</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nسلوک معنوی ابن عربی</td>\n",
       "      <td>{ویلیام سی. چیتیک, جان جی سولیوان}</td>\n",
       "      <td>{حسین مریدی}</td>\n",
       "      <td>{انتشارات جامی}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100 پرسش و پاسخ درباره اختلال افسردگی یا دو ق...</td>\n",
       "      <td>{لیندا کاکرورتی}</td>\n",
       "      <td>{nan}</td>\n",
       "      <td>{نشر دانژه}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100 پرسش و پاسخ درباره نقص توجه/بیش فعالی بزر...</td>\n",
       "      <td>{آوا تی آلبرشت}</td>\n",
       "      <td>{nan}</td>\n",
       "      <td>{نشر دانژه}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 شیوه اساسی برای ترکیب سواد رسانه و تفکر ان...</td>\n",
       "      <td>{سیدنی شیب}</td>\n",
       "      <td>{مرجان اردشیرزاده}</td>\n",
       "      <td>{سمندر}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 اشتباه بچه گانه ای که زوج ها در زندگی زناش...</td>\n",
       "      <td>{پل دبلیو کلمن}</td>\n",
       "      <td>{آرمانوش باباخانیانس}</td>\n",
       "      <td>{دنیس}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                              \\nسلوک معنوی ابن عربی   \n",
       "1   100 پرسش و پاسخ درباره اختلال افسردگی یا دو ق...   \n",
       "2   100 پرسش و پاسخ درباره نقص توجه/بیش فعالی بزر...   \n",
       "3   12 شیوه اساسی برای ترکیب سواد رسانه و تفکر ان...   \n",
       "4   25 اشتباه بچه گانه ای که زوج ها در زندگی زناش...   \n",
       "\n",
       "                               author             translator        publisher  \n",
       "0  {ویلیام سی. چیتیک, جان جی سولیوان}           {حسین مریدی}  {انتشارات جامی}  \n",
       "1                    {لیندا کاکرورتی}                  {nan}      {نشر دانژه}  \n",
       "2                     {آوا تی آلبرشت}                  {nan}      {نشر دانژه}  \n",
       "3                         {سیدنی شیب}     {مرجان اردشیرزاده}          {سمندر}  \n",
       "4                     {پل دبلیو کلمن}  {آرمانوش باباخانیانس}           {دنیس}  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c4b8b167c9f9957e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:03:51.146651Z",
     "start_time": "2024-07-07T15:03:50.939475Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.merge(prepared_data, crawled_data, left_on='bookname', right_on='name')\n",
    "data = data.drop(columns=['bookname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2bf38c6a6e056e2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:03:51.875578Z",
     "start_time": "2024-07-07T15:03:51.861709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data: 69829\n",
      "Crawled data: 166540\n",
      "Merged data: 88828\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>translator</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اسم کتاب   No one writes to the Colonel\\nترجمش...</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "      <td>{گابریل گارسیا مارکز}</td>\n",
       "      <td>{نازنین نوذری}</td>\n",
       "      <td>{موسسه فرهنگی هنری نوروز هنر}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طاقچه عزیز،نام کتاب\"کسی به سرهنگ نامه نمینویسد...</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "      <td>{گابریل گارسیا مارکز}</td>\n",
       "      <td>{نازنین نوذری}</td>\n",
       "      <td>{موسسه فرهنگی هنری نوروز هنر}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بنظرم این اثر مارکز خیلی از صد سال تنهایی که ب...</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "      <td>{گابریل گارسیا مارکز}</td>\n",
       "      <td>{نازنین نوذری}</td>\n",
       "      <td>{موسسه فرهنگی هنری نوروز هنر}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>به نظر کتاب خوبی میومد اما من از ترجمش خوشم نی...</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "      <td>{گابریل گارسیا مارکز}</td>\n",
       "      <td>{نازنین نوذری}</td>\n",
       "      <td>{موسسه فرهنگی هنری نوروز هنر}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>کتاب خوبی است</td>\n",
       "      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>\n",
       "      <td>{گابریل گارسیا مارکز}</td>\n",
       "      <td>{نازنین نوذری}</td>\n",
       "      <td>{موسسه فرهنگی هنری نوروز هنر}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0  اسم کتاب   No one writes to the Colonel\\nترجمش...   \n",
       "1  طاقچه عزیز،نام کتاب\"کسی به سرهنگ نامه نمینویسد...   \n",
       "2  بنظرم این اثر مارکز خیلی از صد سال تنهایی که ب...   \n",
       "3  به نظر کتاب خوبی میومد اما من از ترجمش خوشم نی...   \n",
       "4                                      کتاب خوبی است   \n",
       "\n",
       "                                name                 author      translator  \\\n",
       "0  سرهنگ کسی ندارد برایش نامه بنویسد  {گابریل گارسیا مارکز}  {نازنین نوذری}   \n",
       "1  سرهنگ کسی ندارد برایش نامه بنویسد  {گابریل گارسیا مارکز}  {نازنین نوذری}   \n",
       "2  سرهنگ کسی ندارد برایش نامه بنویسد  {گابریل گارسیا مارکز}  {نازنین نوذری}   \n",
       "3  سرهنگ کسی ندارد برایش نامه بنویسد  {گابریل گارسیا مارکز}  {نازنین نوذری}   \n",
       "4  سرهنگ کسی ندارد برایش نامه بنویسد  {گابریل گارسیا مارکز}  {نازنین نوذری}   \n",
       "\n",
       "                       publisher  \n",
       "0  {موسسه فرهنگی هنری نوروز هنر}  \n",
       "1  {موسسه فرهنگی هنری نوروز هنر}  \n",
       "2  {موسسه فرهنگی هنری نوروز هنر}  \n",
       "3  {موسسه فرهنگی هنری نوروز هنر}  \n",
       "4  {موسسه فرهنگی هنری نوروز هنر}  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Prepared data: {len(prepared_data)}\\nCrawled data: {len(crawled_data)}\\nMerged data: {len(data)}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2c4ba25b104a2188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T15:04:24.057647Z",
     "start_time": "2024-07-07T15:04:24.014272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unavailable books: 8102\n",
      "Is the number of unavailable books equal to the difference between prepared and merged data?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "crawled_books = set(crawled_data['name'].values)\n",
    "prepared_books = prepared_data['bookname'].values\n",
    "\n",
    "unavailable_books = [book for book in prepared_books if book not in crawled_books]\n",
    "print(f'Unavailable books: {len(unavailable_books)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280620f9fb070b1a",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44b9919ee2a11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
